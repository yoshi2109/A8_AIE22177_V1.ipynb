{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1iM2JVuVoINu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Sbcm5idDoINx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def entropy(y):\n",
        "    \"\"\"Calculate the entropy of a dataset.\"\"\"\n",
        "    # Count the occurrences of each class\n",
        "    counts = np.bincount(y)\n",
        "    probabilities = counts / len(y)\n",
        "    # Filter out zero probabilities to avoid math error in np.log2\n",
        "    probabilities = probabilities[probabilities > 0]\n",
        "    # Calculate entropy\n",
        "    return -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "def conditional_entropy(x, y):\n",
        "    \"\"\"Calculate the conditional entropy of y given x.\"\"\"\n",
        "    # Entropy accumulator\n",
        "    entropy_acc = 0\n",
        "    # Iterate over each category in x to calculate conditional entropy\n",
        "    for value in np.unique(x):\n",
        "        # Subset y based on the current category in x\n",
        "        subset_y = y[x == value]\n",
        "        # Calculate the probability of the current category\n",
        "        probability = len(subset_y) / len(y)\n",
        "        # Add to the conditional entropy accumulator\n",
        "        entropy_acc += probability * entropy(subset_y)\n",
        "    return entropy_acc\n",
        "\n",
        "def information_gain(X, y, feature_index):\n",
        "    \"\"\"Calculate the Information Gain of a feature.\"\"\"\n",
        "    # Calculate the entropy of the entire dataset\n",
        "    total_entropy = entropy(y)\n",
        "    # Calculate the conditional entropy of y given the feature\n",
        "    feature_entropy = conditional_entropy(X[:, feature_index], y)\n",
        "    # Calculate Information Gain\n",
        "    return total_entropy - feature_entropy\n",
        "\n",
        "def find_root_node(X, y):\n",
        "    \"\"\"Find the feature index that should be used as the root node.\"\"\"\n",
        "    # Calculate information gain for each feature\n",
        "    gains = [information_gain(X, y, feature_index) for feature_index in range(X.shape[1])]\n",
        "    # Find the index of the feature with the highest information gain\n",
        "    return np.argmax(gains)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iwIiGcndoINx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def bin_continuous_values(x, n_bins=3, strategy=\"width\"):\n",
        "    \"\"\"Bin continuous values into categorical.\"\"\"\n",
        "    if strategy == \"width\":\n",
        "        # Equal width binning\n",
        "        # Define bins with equal width\n",
        "        bins = np.linspace(np.min(x), np.max(x), n_bins + 1)\n",
        "        # Digitize the continuous values based on the defined bins\n",
        "        return np.digitize(x, bins) - 1\n",
        "    elif strategy == \"frequency\":\n",
        "        # Equal frequency binning (quantiles)\n",
        "        # Determine quantiles to split the data into equal frequency bins\n",
        "        quantiles = np.quantile(x, np.linspace(0, 1, n_bins + 1))\n",
        "        # Digitize the continuous values based on the quantiles\n",
        "        return np.digitize(x, quantiles) - 1\n",
        "    else:\n",
        "        # Raise an error if an unsupported strategy is provided\n",
        "        raise ValueError(\"Unsupported binning strategy\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BbRHxkvcoINy"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, value=None, left=None, right=None):\n",
        "        self.feature_index = feature_index  # Index of the feature to split on\n",
        "        self.threshold = threshold          # Threshold value for binary splits\n",
        "        self.value = value                  # Value to return if this node is a leaf\n",
        "        self.left = left                    # Left child (for binary splits)\n",
        "        self.right = right                  # Right child (for binary splits)\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Consider stopping criteria such as max depth or pure leaf nodes\n",
        "\n",
        "        # Stopping criteria\n",
        "        if depth == self.max_depth or len(np.unique(y)) == 1:\n",
        "            # Return leaf node with majority class\n",
        "            return Node(value=np.bincount(y).argmax())\n",
        "\n",
        "        # Find the best split using information gain\n",
        "        best_feature_index = find_root_node(X, y)\n",
        "        best_information_gain = information_gain(X, y, best_feature_index)\n",
        "\n",
        "        # Check if no information gain\n",
        "        if best_information_gain == 0:\n",
        "            # Return leaf node with majority class\n",
        "            return Node(value=np.bincount(y).argmax())\n",
        "\n",
        "        # Split the dataset based on the best feature and threshold\n",
        "        best_threshold = np.median(X[:, best_feature_index])\n",
        "        left_indices = X[:, best_feature_index] <= best_threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        # Create and return the current node\n",
        "        return Node(feature_index=best_feature_index, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            node = self.root\n",
        "            while node.left:\n",
        "                if sample[node.feature_index] <= node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "            predictions.append(node.value)\n",
        "        return predictions\n",
        "\n",
        "# Read the dataset\n",
        "data = pd.read_csv(\"/loan-train.csv\")\n",
        "\n",
        "# Preprocess the data\n",
        "\n",
        "# Handling missing values\n",
        "data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
        "\n",
        "# Convert categorical variables into numerical ones using one-hot encoding\n",
        "data = pd.get_dummies(data)\n",
        "\n",
        "# Split data into features and target variable\n",
        "X = data.drop(columns=['Loan_Status_Y']).values\n",
        "y = data['Loan_Status_Y'].values\n",
        "\n",
        "# Initialize and train the decision tree\n",
        "tree = DecisionTree(max_depth=5)\n",
        "tree.fit(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrsVCn-GyFkF",
        "outputId": "88bdafdd-1f8c-4013-dd9e-f362ce27c426"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}